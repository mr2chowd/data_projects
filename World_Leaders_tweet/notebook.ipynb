{"cells":[{"cell_type":"markdown","metadata":{"dc":{"key":"4"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 1. World Leaders\n","\n","<p>Are you up for the challenge of classifying text, specifically tweets? In this notebook, we'll explore the realm of social media text classification and delve into the task of properly categorizing tweets from two prominent North American politicians: Donald Trump and Justin Trudeau.</p>\n","<p>Classifying tweets presents unique challenges in natural language processing due to their brevity. Additionally, various platform-specific conventions such as mentions, #hashtags, emojis, links, and short-hand phrases (ikr?) can make the task more complex. But fear not! We will overcome these hurdles and construct a valuable classifier that can distinguish between these two prolific tweeters. Let's get started!</p>\n","<p>To begin, we'll import all the necessary tools from scikit-learn. This includes data vectorization techniques like <code>CountVectorizer</code> and <code>TfidfVectorizer</code>. We'll also import essential models, such as <code>MultinomialNB</code> from the <code>naive_bayes</code> module, <code>LinearSVC</code> from the <code>svm</code> module, and <code>PassiveAggressiveClassifier</code> from the <code>linear_model</code> module. Lastly, to evaluate and optimize our model, we'll need <code>sklearn.metrics</code> and the utility functions <code>train_test_split</code> and <code>GridSearchCV</code> from the <code>model_selection</code> module.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"4"},"tags":["sample_code"]},"outputs":[],"source":["# Set seed for reproducibility\n","import random; random.seed(53)\n","\n","# Import all we need from sklearn\n","# ... YOUR CODE FOR TASK 1 ...\n","# ... YOUR CODE FOR TASK 1 ...\n","# ... YOUR CODE FOR TASK 1 ...\n","# ... YOUR CODE FOR TASK 1 ...\n","# ... YOUR CODE FOR TASK 1 ..."]},{"cell_type":"markdown","metadata":{"dc":{"key":"11"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 2. Transforming our collected data\n","<p>To kick off, we will utilize a collection of tweets gathered in November 2017, presented in CSV format. Employing Pandas DataFrame, we will import and subsequently process the data using scikit-learn.</p>\n","<p>Given that the Twitter API furnished the data without segregating it into test and training sets, we will perform this division ourselves. By employing <code>train_test_split()</code> with <code>random_state=53</code> and specifying a test size of 0.33, we can ensure adequate test data while attaining consistent results regardless of the code's location or execution time, akin to our methodology in the DataCamp course.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"11"},"tags":["sample_code"]},"outputs":[],"source":["import pandas as pd\n","\n","# Load data\n","tweet_df = ...\n","\n","# Create target\n","y = ...\n","\n","# Split training and testing data\n","X_train, X_test, y_train, y_test = ..."]},{"cell_type":"markdown","metadata":{"dc":{"key":"18"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 3. Vectorize the tweets\n","<p>We have the training and testing data all set up, but we need to create vectorized representations of the tweets in order to apply machine learning.</p>\n","<p>To do so, we will utilize the <code>CountVectorizer</code> and <code>TfidfVectorizer</code> classes which we will first need to fit to the data.</p>\n","<p>Once this is complete, we can start modeling with the new vectorized tweets!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"18"},"tags":["sample_code"]},"outputs":[],"source":["# Initialize count vectorizer\n","count_vectorizer = ...\n","\n","# Create count train and test variables\n","count_train = ...\n","count_test = ...\n","\n","# Initialize tfidf vectorizer\n","tfidf_vectorizer = ...\n","\n","# Create tfidf train and test variables\n","tfidf_train = ...\n","tfidf_test = ..."]},{"cell_type":"markdown","metadata":{"dc":{"key":"25"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 4. Training a multinomial naive Bayes model\n","<p>Now that we have the data in vectorized form, we can train the first model. Investigate using the Multinomial Naive Bayes model with both the <code>CountVectorizer</code> and <code>TfidfVectorizer</code> data. Which do will perform better? How come?</p>\n","<p>To assess the accuracies, we will print the test sets accuracy scores for both models.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"25"},"tags":["sample_code"]},"outputs":[],"source":["# Create a MulitnomialNB model\n","tfidf_nb = ...\n","\n","# ... Train your model here ...\n","\n","# Run predict on your TF-IDF test data to get your predictions\n","tfidf_nb_pred = ...\n","\n","# Calculate the accuracy of your predictions\n","tfidf_nb_score = ...\n","\n","# Create a MulitnomialNB model\n","count_nb = ...\n","# ... Train your model here ...\n","\n","# Run predict on your count test data to get your predictions\n","count_nb_pred = ...\n","\n","# Calculate the accuracy of your predictions\n","count_nb_score = ...\n","\n","print('NaiveBayes Tfidf Score: ', tfidf_nb_score)\n","print('NaiveBayes Count Score: ', count_nb_score)"]},{"cell_type":"markdown","metadata":{"dc":{"key":"32"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 5. Evaluating our model using a confusion matrix\n","<p>We see that the TF-IDF model performs better than the count-based approach. Based on what we know from the NLP fundamentals course, why might that be? We know that TF-IDF allows unique tokens to have a greater weight - perhaps tweeters are using specific important words that identify them! Let's continue the investigation.</p>\n","<p>For classification tasks, an accuracy score doesn't tell the whole picture. A better evaluation can be made if we look at the confusion matrix, which shows the number correct and incorrect classifications based on each class. We can use the metrics, True Positives, False Positives, False Negatives, and True Negatives, to determine how well the model performed on a given class. How many times was Trump misclassified as Trudeau?</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"32"},"tags":["sample_code"]},"outputs":[],"source":["%matplotlib inline\n","\n","from datasets.helper_functions import plot_confusion_matrix\n","\n","# Calculate the confusion matrices for the tfidf_nb model and count_nb models\n","tfidf_nb_cm = ...\n","count_nb_cm = ...\n","\n","# Plot the tfidf_nb_cm confusion matrix\n","plot_confusion_matrix(tfidf_nb_cm, classes=..., title=\"TF-IDF NB Confusion Matrix\")\n","\n","# Plot the count_nb_cm confusion matrix without overwriting the first plot \n","plot_confusion_matrix(..., classes=..., title=..., figure=1)"]},{"cell_type":"markdown","metadata":{"dc":{"key":"39"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 6. Trying out another classifier: Linear SVC\n","<p>So the Bayesian model only has one prediction difference between the TF-IDF and count vectorizers -- fairly impressive! Interestingly, there is some confusion when the predicted label is Trump but the actual tweeter is Trudeau. If we were going to use this model, we would want to investigate what tokens are causing the confusion in order to improve the model. </p>\n","<p>Now that we've seen what the Bayesian model can do, how about trying a different approach? <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\">LinearSVC</a> is another popular choice for text classification. Let's see if using it with the TF-IDF vectors improves the accuracy of the classifier!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"39"},"tags":["sample_code"]},"outputs":[],"source":["# Create a LinearSVM model\n","tfidf_svc = ...\n","\n","# ... Train your model here ...\n","\n","# Run predict on your tfidf test data to get your predictions\n","tfidf_svc_pred = ...\n","\n","# Calculate your accuracy using the metrics module\n","tfidf_svc_score = ...\n","\n","print(\"LinearSVC Score:   %0.3f\" % tfidf_svc_score)\n","\n","# Calculate the confusion matrices for the tfidf_svc model\n","svc_cm = ...\n","\n","# Plot the confusion matrix using the plot_confusion_matrix function\n","plot_confusion_matrix(svc_cm, classes=..., title=\"TF-IDF LinearSVC Confusion Matrix\")"]},{"cell_type":"markdown","metadata":{"dc":{"key":"46"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 7. Introspecting our top model\n","<p>Wow, the LinearSVC model is even better than the Multinomial Bayesian one. Nice work! Via the confusion matrix we can see that, although there is still some confusion where Trudeau's tweets are classified as Trump's, the False Positive rate is better than the previous model. So, we have a performant model, right? </p>\n","<p>We might be able to continue tweaking and improving all of the previous models by learning more about parameter optimization or applying some better preprocessing of the tweets. </p>\n","<p>Now let's see what the model has learned. Using the LinearSVC Classifier with two classes (Trump and Trudeau) we can sort the features (tokens), by their weight and see the most important tokens for both Trump and Trudeau. What are the most Trump-like or Trudeau-like words? Did the model learn something useful to distinguish between these two men? </p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"46"},"tags":["sample_code"]},"outputs":[],"source":["from datasets.helper_functions import plot_and_return_top_features\n","\n","# Import pprint from pprint\n","from pprint ...\n","\n","# Get the top features using the plot_and_return_top_features function and your top model and tfidf vectorizer\n","top_features = ...\n","\n","# pprint the top features\n","pprint(...)"]},{"cell_type":"markdown","metadata":{"dc":{"key":"53"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"source":["## 8. Bonus: can you write a Trump or Trudeau tweet?\n","\n","<p>Upon analyzing the results, it appears that our model has successfully identified that Trudeau tends to tweet in French!</p>\n","<p>Now, it's your turn to craft a tweet using this newfound knowledge to challenge the model! Utilize the displayed list or plot above to make educated guesses on the words that will categorize your text as either Trump or Trudeau. Can you cleverly deceive the model into believing that you are either Trump or Trudeau?</p>\n","<p>If you happen to be fluent in French, don't hesitate to compose your Trudeau-style tweet in French! As observed, these French words are common and often known as \"stop words.\" Although you have the option to preprocess the tweets by removing both English and French stop words, it may reduce the model's accuracy since Trudeau is the sole French-speaker in the dataset. If the dataset included multiple French speakers, this preprocessing step would prove valuable.</p>\n","<p>For future research on this dataset, some potential avenues to explore are:</p>\n","<ul>\n","<li>Incorporating additional preprocessing techniques (e.g., removing URLs or French stop words) and studying their impact on the model</li>\n","<li>Enhancing both the Bayesian and LinearSVC models by using GridSearchCV to identify optimal parameters</li>\n","<li>Conducting introspection on the Bayesian model to identify words that lean towards either Trump's or Trudeau's writing style</li>\n","<li>Enriching the dataset with more recent tweets using tweepy and subsequently retraining the model</li>\n","</ul>\n","<p>Best of luck as you compose your impersonation tweets â€“ don't forget to share them on Twitter if you'd like!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"dc":{"key":"53"},"tags":["sample_code"]},"outputs":[],"source":["# Write two tweets as strings, one which you want to classify as Trump and one as Trudeau\n","trump_tweet = ...\n","trudeau_tweet = ...\n","\n","# Vectorize each tweet using the TF-IDF vectorizer's transform method\n","# Note: `transform` needs the string in a list object (i.e. [trump_tweet])\n","trump_tweet_vectorized = ...\n","trudeau_tweet_vectorized = ...\n","\n","# Call the predict method on your vectorized tweets\n","trump_tweet_pred = ...\n","trudeau_tweet_pred = ...\n","\n","print(\"Predicted Trump tweet\", trump_tweet_pred)\n","print(\"Predicted Trudeau tweet\", trudeau_tweet_pred)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":2}
